{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de Recomendación - Netflix Prize Challenge\n",
    "\n",
    "En este notebook vamos a implementar un sistema de recomendación a través de un filtro colaborativo.\n",
    "\n",
    "Algunas referencias útiles, además de las mencionadas en la presentación:\n",
    "* https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system\n",
    "* https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101\n",
    "\n",
    "El dataset puede ser descargado [acá](https://www.kaggle.com/netflix-inc/netflix-prize-data)\n",
    "\n",
    "Importante leer el archivo `README.md` para la descripción de los archivos.\n",
    "\n",
    "## **Big Data**\n",
    "\n",
    "\n",
    "Uno de los desafíos que plantea este dataset es que es bastante \"grande\". Esto quiere decir que, si lo cargamos completo, ocupa bastante lugar en nuestra memoria RAM. Además, cada tarea puede llevar mucho tiempo. Entonces, es necesario plantear una estrategia para abordarlo. Existen varias posibilidades, mencionamos algunas:\n",
    "1. Recortar una parte del dataset con la que sí podamos trabajar. Esta parte tiene que ser lo suficientemente representativa del set original. Para estar seguros de ello es fundamental hacer una buena exploración de datos. Con ese recorte, entrenamos y evaluamos nuestro modelo, y optimizamos parámetros (CV). Una vez que ya estamos seguro de que nuestro flujo de trabajo es apropiado, podemos probar agrandar la porción de datos con la que entrenamos o utilizar otro recorte del dataset. Eventualmente, podemos llegar a usar todo el dataset para entrenar y evaluar si nuestra computadora lo permite. **NOTA**: el recorte se hace para que se pueda cargar en memoria los datos pero también para que cada iteración lleve un tiempo razonable.\n",
    "2. **Aprendizaje incremental**: algunos modelos puede ser entrenados mostrándoles el dataset de a pedazos. Es decir, no necesitan ver todo el dataset a la vez. Un ejemplo son las redes neuronales, que \"ven\" muchas pasadas del dataset en *epochs* y *minibatches*. Algunos modelos en Scikit-learn tienen la función `partial_fit` que permite hacer eso. Pueden leer un poco al respecto [acá](https://scikit-learn.org/stable/modules/computing.html). Pandas también tiene funciones que permiten cargar el dataset de a trozos.\n",
    "3. Utilizar servicios en la nube. Esta opción no es excluyente con las anteriores. Antes de utilizar algún entorno en la nube, está bueno haber hecho pruebas en nuestra computadora y ya haber optimizado bastante el flujo de trabajo. Recuerden que los servicios en la nube se pagan.\n",
    "4. Existen librerías orientadas a trabajar con grandes datos. Un ejemplo es [Dask](https://dask.org/).\n",
    "\n",
    "## 0. Algunos preliminares\n",
    "\n",
    "Mientras miran el estado de la memoria RAM, crear un arreglo 2-D de unos en numpy de forma (10000,10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "unos = np.ones((N,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unos # Borra la variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tipo de dato es `ones`?¿Y sus elementos? Crear el mismo arreglo, pero convertir los elementos en *np.int8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos = np.ones((N,N)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por las dudas reiniciar el Kernel antes de continuar y correr a partir de la sección siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos y preparación del Dataset\n",
    "\n",
    "Vamos a empezar cargando uno de los archivos con calificaciones para explorarlo. Como son archivos grandes y van a ocupar bastante lugar en memoria, no vamos a cargar la última columna con fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gc #garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    df = pd.read_csv(name, header = None, names = ['User','Rating'], usecols = [0,1])\n",
    "    \n",
    "    # A veces forzar un tipo de dato hace que se ahorre mucho lugar en memoria.\n",
    "    df['Rating'] = df['Rating']#.astype(float) \n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = load_data('Datasets/netflix-prize-data/combined_data_1.txt')\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sabemos a qué película corresponde cada calificación? Contar cuántas películas hay en `df1` e identificarlas. Para ello, cargamos `movie_titles.csv`. Como no nos interesa el año, no lo traemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = pd.read_csv('Datasets/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\",index_col = 0, header = None, usecols = [0,2], names = ['Movie_Id', 'Name'])\n",
    "df_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, podemos obtener el nombre de una película dado su Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = 1\n",
    "print(df_title.loc[movie_id].Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contar cuántos identificadores hay, vamos a usar la siguiente información: al lado del identificador de la película, la columna `Rating` de `df1` tiene un `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ids_df1 = df1.User[df1.Rating.isna()].values\n",
    "print(movies_ids_df1)\n",
    "print(len(movies_ids_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En qué formato está? Si queremos usarlo para pasar de identificador al nombre, debemos llevarlo a enteros. Asumimos que no hay ningun repetido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ids_df1 = np.arange(1,len(movies_ids_df1) + 1)\n",
    "print(movies_ids_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Id\n",
    "\n",
    "Intentaremos agregar una columna al Dataframe con el Id de la película a la que corresponde la calificación. Para ello, vamos a necesitar saber dónde están ubicados los identificadores.\n",
    "\n",
    "Primero, seleccionamos los índices donde aparecen los movies_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nan = pd.DataFrame(pd.isnull(df1.Rating))\n",
    "df1_nan = df1_nan[df1_nan['Rating'] == True]\n",
    "idx_movies_ids = df1_nan.index.values\n",
    "print(idx_movies_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos crear un vector de tantas instancias como `df1`, donde en cada lugar esté movie_id a cual corresponde la calificación. Como tenemos los índices donde está cada movie_id, podemos obtener cuántas calificaciones hay de cada película."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el indice de la ultima instancia del dataframe\n",
    "idx_movies_ids = np.append(idx_movies_ids,df1.shape[0])\n",
    "cantidad_criticas = np.diff(idx_movies_ids)\n",
    "cantidad_criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_movie_id = np.array([])\n",
    "for i in range(cantidad_criticas.size):\n",
    "    aux = np.full(cantidad_criticas[i], movies_ids_df1[i])\n",
    "    columna_movie_id = np.concatenate((columna_movie_id, aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos esa columna al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['movie_id'] = columna_movie_id\n",
    "del columna_movie_id\n",
    "\n",
    "df1.dropna(inplace = True)\n",
    "df1['User'] = df1['User'].astype(int)\n",
    "df1['movie_id'] = df1['movie_id'].astype(np.int16)\n",
    "df1['Rating'] = df1['Rating'].astype(np.int8)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya contamos con un dataframe con calificaciones de usuarios a películas.\n",
    "\n",
    "Una opción es guardar el dataset modificado en nuevo archivo y, a partir de ahora, trabajar con esa versión. Esto hará que no tengamos que hacer el preprocesamiento cada vez que empecemos a trabajar y, además, ahorrarnos toda la \"basura\" que Python pueda ir dejando en la RAM.\n",
    "\n",
    "**Ejercicio**: guardar el dataset modificado en un nuevo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df1.to_csv('Datasets/netflix-prize-data/combined_data_1_con_movie_id.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración del Dataset\n",
    "\n",
    "Responder las siguientes preguntas, siempre que se pueda con un lindo gráfico (¡pensar bien cómo!):\n",
    "\n",
    "1. ¿Cuántos usuarios únicos hay?\n",
    "2. ¿Cuántas películas calificó cada usuario?\n",
    "3. ¿Cómo es la distribución de las calificaciones?¿Pueden concluir algo de ese gráfico?\n",
    "4. ¿Cuál es la película con más calificaciones?¿Cuántas tiene?¿Y la que menos calificaciones tiene?\n",
    "\n",
    "Arrancamos abriendo el dataset ya modificado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Datasets/netflix-prize-data/combined_data_1_con_movie_id.csv', dtype={'Rating': np.int8, 'movie_id': np.int16})\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y los títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = pd.read_csv('Datasets/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\",index_col = 0, header = None, usecols = [0,2], names = ['Movie_Id', 'Name'])\n",
    "df_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuántos usuarios únicos hay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1['User'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Cuántas películas calificó cada usuario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_by_users = df1.groupby(['User']).count()\n",
    "df1_by_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df1_by_users.Rating, log= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Cómo es la distribución de las calificaciones?¿Pueden concluir algo de ese gráfico?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Rating'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Cuál es la película con más calificaciones?¿Cuántas tiene?¿Y la que menos calificaciones tiene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_by_movies = df1.groupby(['movie_id']).count()\n",
    "df1_by_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = df1_by_movies['User'].idxmax()\n",
    "print(df_title.loc[idx_max].Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pelicula con menos calificaciones\n",
    "## COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "Ordenamos las películas por popularidad y le ponemos el nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_by_movies = df1.groupby(['movie_id']).count()\n",
    "df1_by_movies.sort_values('User', ascending = False, inplace = True)\n",
    "df1_by_movies['Vistos'] = df1_by_movies['User']\n",
    "df1_by_movies.drop(columns = ['User','Rating'], inplace = True)\n",
    "df1_by_movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_by_movies['Titulo'] = df_title.loc[df1_by_movies.index].Name\n",
    "df1_by_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional: filtrar películas con pocos ratings\n",
    "\n",
    "Primero, veamos cómo es la distribución de `Vistos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "# df1_by_movies.Vistos.hist(log = True, bins = 50)\n",
    "df1_by_movies.Vistos[df1_by_movies.Vistos<1000].hist(log = True, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a elegir un umbral y descartar aquellas películas que tengan menos vistos que ese umbral. Primero, creamos un arreglo con los movies_ids de las películas que queremos descartar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral = 1000\n",
    "mascara_pocos_vistos = df1_by_movies.Vistos<umbral\n",
    "peliculas_pocos_vistos = mascara_pocos_vistos[mascara_pocos_vistos].index.values\n",
    "print(len(peliculas_pocos_vistos), peliculas_pocos_vistos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tiramos de `df1` aquellas calificaciones que correspondan a las películas que queremos descartar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascara_descartables = df1.movie_id.isin(peliculas_pocos_vistos)\n",
    "mascara_descartables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)\n",
    "df1 = df1[~mascara_descartables]\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento\n",
    "\n",
    "Para entrenar el sistema de recomendación vamos a usar la biblioteca Surprise. Recomendamos tener abierta la [documentación](https://surprise.readthedocs.io/en/stable/getting_started.html) a medida que van a través de este notebook.\n",
    "\n",
    "### 3.1 Dataset y Train/test split\n",
    "\n",
    "Primero, llevamos el dataset al formato que le gusta a la biblioteca. ¿En qué orden tienen que estar los atributos?. Investigar qué hace la clase `Reader` y cuáles son sus parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, creamos el `Dataset` de Surprise usando `Dataset.load_from_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_filas = 100000 # Limitamos el dataset a N_filas\n",
    "\n",
    "data = Dataset.load_from_df(df1[['User', 'movie_id', 'Rating']][:N_filas], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo les parece que es mejor hacer el split?¿Dejando películas en test, usuarios o combinaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento\n",
    "\n",
    "Vamos a entrenar un algoritmo SVD. Explorar sus parámetros y su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "algo = SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos sobre el `trainset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y predecimos sobre el `testset`. Notar que para predecir sobre un conjunto de test se usa la función `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorar las característica de `predictions` y alguno de sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, si queremos predecir para un usuario y una película en particular, usamos la función `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(1328945,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos un usuario, veamos cuáles películas le gustaron y cuáles les recomienda el sistema.\n",
    "\n",
    "Películas que le gustaron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario = 1539350\n",
    "rating = 5   # le pedimos peliculas a las que haya puesto 4 o 5 estrellas\n",
    "df_user = df1[(df1['User'] == usuario) & (df1['Rating'] >= rating)]\n",
    "df_user = df_user.reset_index(drop=True)\n",
    "df_user['Name'] = df_title['Name'].loc[df_user.movie_id].values\n",
    "df_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos donde vamos a guardar las recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario = df_title.iloc[:4499].copy()\n",
    "print(recomendaciones_usuario.shape)\n",
    "recomendaciones_usuario.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos del dataframe todas las películas que ya sabemos que vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario_vistas = df1[df1['User'] == usuario]\n",
    "print(usuario_vistas.shape)\n",
    "usuario_vistas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Sacamos las que filtramos (si filtramos)\n",
    "    recomendaciones_usuario.drop(peliculas_pocos_vistos, inplace = True)\n",
    "recomendaciones_usuario.drop(usuario_vistas.movie_id, inplace = True)\n",
    "recomendaciones_usuario = recomendaciones_usuario.reset_index()\n",
    "recomendaciones_usuario.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y hacemos las recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario['Estimate_Score'] = recomendaciones_usuario['Movie_Id'].apply(lambda x: algo.predict(usuario, x).est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario = recomendaciones_usuario.sort_values('Estimate_Score', ascending=False)\n",
    "print(recomendaciones_usuario.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluación\n",
    "\n",
    "Para el conjunto de `testset`, evaluamos el error RMSE entre las predicciones y las verdaderas calificaciones que le habían dado a las películas. Para eso, buscar en la documentación cómo se hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimización de parámetros\n",
    "\n",
    "**Ejercicio**: hacer un gráfico del desempeño del modelo en función del número de factores del `SVD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "rmse_test_means = []\n",
    "factores = COMPLETAR\n",
    "\n",
    "for COMPLETAR in COMPLETAR:\n",
    "    print(COMPLETAR)\n",
    "    algo = SVD(n_factors=COMPLETAR)\n",
    "    cv = COMPLETAR(COMPLETAR, COMPLETAR, measures=['RMSE'], cv = 3, verbose=True)\n",
    "    rmse_test_means.append(np.mean(cv['test_rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(COMPLETAR, COMPLETAR)\n",
    "plt.xlabel('Numero de factores')\n",
    "plt.ylabel('Error RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: recordar que, cuando entrenamos un `SVD`, estamos usando descenso por gradiente para minimizar una función de costo. Usar `GridSearchCV` para buscar valores óptimos para los siguientes parámetros (tres por parámetros, utilizar los valores default de referencia): `n_factors`, `n_epochs`, `lr_all` y `reg_all`. Estudiar qué representa cada uno de ellos mientras esperan. Tomarse un café."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'COMPLETAR': COMPLETAR,'COMPLETAR': COMPLETAR], 'COMPLETAR': COMPLETAR,\n",
    "              'COMPLETAR': COMPLETAR}\n",
    "gs = GridSearchCV(SVD, COMPLETAR, measures=['rmse'], cv=3, n_jobs = -1)\n",
    "gs.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agrandando el Dataset\n",
    "\n",
    "Ahora vamos a sumar al dataset el resto de las calificaciones que no usamos.\n",
    "\n",
    "Como corremos el riesgo de que se nos llene la memoria RAM, vamos a hacerlo de a poco y con cuidado. Arrancamos agregando las calificaciones que hay en `combined_data_2.txt`.\n",
    "\n",
    "0. Reiniciar el Kernel\n",
    "1. Abrir el archivo `combined_data_2.txt` con la función `load_data`.\n",
    "2. Agregar una columna con el movie_id al que corresponde las calificaciones. Si se animan, pueden crear una función que realice este paso.\n",
    "3. Opcional: filtrar películas con pocas calificaciones\n",
    "4. Abrir el archivo donde ya está procesado `combined_data_1.txt`. \n",
    "5. Agregar al final las nuevas calificaciones y guardarlo en un nuevo archivo.\n",
    "\n",
    "\n",
    "Una vez que estén contentos con el procedimientos, repetir los pasos anteriores para los archivos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gc #garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1\n",
    "\n",
    "def load_data(name):\n",
    "    df = pd.read_csv(name, header = None, names = ['User','Rating'], usecols = [0,1])\n",
    "    \n",
    "    # A veces forzar un tipo de dato hace que se ahorre mucho lugar en memoria.\n",
    "    df['Rating'] = df['Rating']#.astype(float) \n",
    "    return df\n",
    "\n",
    "\n",
    "df2 = load_data(COMPLETAR)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el indice de la ultima instancia del dataframe\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_movie_id = np.array([])\n",
    "for i in range(cantidad_criticas.size):\n",
    "    aux = np.full(cantidad_criticas[i], movies_ids_df2[i])\n",
    "    columna_movie_id = np.concatenate((columna_movie_id, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['movie_id'] = columna_movie_id\n",
    "del columna_movie_id\n",
    "\n",
    "df2.dropna(inplace = True)\n",
    "df2['User'] = df2['User'].astype(int)\n",
    "df2['movie_id'] = df2['movie_id'].astype(np.int16)\n",
    "df2['Rating'] = df2['Rating'].astype(np.int8)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.\n",
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.\n",
    "\n",
    "df = df1.copy()\n",
    "del df1\n",
    "df = df.append(df2)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeamos que estén todas las películas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peliculas_presentes = df.movie_id.unique()\n",
    "peliculas_presentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((peliculas_presentes - np.arange(1,9210 + 1)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y guardamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    df.to_csv('Datasets/netflix-prize-data/combined_data_1y2_con_movie_id.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "A lo largo del notebook vamos a trabajar con el siguiente dataset:\n",
    "\n",
    "https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/\n",
    "\n",
    "El objetivo es que se familiaricen con algunas herramientas típicas del Procesamiento del Lenguaje Natural (NLP por sus siglas en inglés). Para ello, es requisito que miren los videos de la plataforma de Acámica y vuelquen lo aprendido acá.\n",
    "\n",
    "La biblioteca fundamental que vamos a usar es NLTK. Probablemente tengan que instalarla. Para ello, pongan `conda install nltk` en la terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos\n",
    "\n",
    "Lo primero que tienen que hacer es fijarse en qué formato están los datos. ¿De qué se trata es formato?¿Cómo se abre? Si googlean, van a ver que hay muchas formas de abrir archivos JSON con Python. Como venimos trabajando con Pandas, googleen \"Open JSON with Pandas\". Prueben esa función. Si les tira un error en el primer intento, googleen el error. Les aseguramos que la respuesta está muy a mano y es muy accesible, no tienen que hacer nada raro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Esto sirve para configurar NLTK. La primera vez puede tardar un poco\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué forma tiene el dataset?¿Cuántas instancias?¿Cuáles son sus columnas?¿Cuántos titulares hay de cada tipo?¿Podemos hablar ya de *features*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26709, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26709 entries, 0 to 26708\n",
      "Data columns (total 3 columns):\n",
      "article_link    26709 non-null object\n",
      "headline        26709 non-null object\n",
      "is_sarcastic    26709 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 626.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(\"is_sarcastic\", data=dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Breve exploración del dataset\n",
    "\n",
    "Elegir una instancia del dataset al azar y seleccionar el *headline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9936 local tcby has missed past 2 logo changes\n"
     ]
    }
   ],
   "source": [
    "index_random = np.random.randint(len(dataset))\n",
    "titular = dataset.headline[index_random]\n",
    "print(index_random, titular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Les parece que es sarcástico?¿Qué características del texto les hace creer - o no - eso? Comprobar si es sarcástico o no imprimiendo en la celda de abajo el valor correspondiente del dataset. (Como la mayoría de los titulares están en inglés y encima refieren a política local, no se preocupen si es una tarea difícil)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9936 1\n"
     ]
    }
   ],
   "source": [
    "print(index_random, dataset.is_sarcastic[index_random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLTK\n",
    "\n",
    "Si es difícil para algunos humanos detectar el sarcasmo, probablemente también lo sea para una computadora. De todas formas, se puede hacer el intento. Para ello, es necesario extraer características de cada texto que nos sirvan para ir apuntando al objetivo. En los videos de Acámica hay muchos ejemplo de herramientas para aplicar. Elegir un titular que les llame la atención y probar las siguientes herramientas:\n",
    "\n",
    "### Tokenización\n",
    "\n",
    "¿Qué es y para qué sirve?¿Cuáles de todas las formas de tokenización presentadas les parece más útil para este problema?\n",
    "\n",
    "1. `sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local tcby has missed past 2 logo changes']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titular_st = nltk.sent_tokenize(dataset.headline[index_random])\n",
    "titular_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local', 'tcby', 'has', 'missed', 'past', '2', 'logo', 'changes']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titular_wt = nltk.word_tokenize(dataset.headline[index_random])\n",
    "titular_wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización\n",
    "\n",
    "¿Qué es y para qué sirve? Notar que varias formas de normalización ya vienen aplicadas en el dataset.\n",
    "\n",
    "1. Stopwords\n",
    "\n",
    "Importar los `stopwords` del inglés e imprimirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Les parece conveniente aplicar todos los stopwords que aparecen en esa lista?\n",
    "\n",
    "Eliminar del titular elegido los stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['local', 'tcby', 'has', 'missed', 'past', '2', 'logo', 'changes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['local', 'tcby', 'missed', 'past', '2', 'logo', 'changes']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(titular_wt)\n",
    "titular_wt_sin_sw = [w for w in titular_wt if w not in stopwords]\n",
    "titular_wt_sin_sw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál o cuáles palabras se fueron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de palabras\n",
    "\n",
    "Dado el titular ya tokenizado por palabras y sin stopwords, usar `nltk` para extrar la frecuencia con que aparece cada palabras. ¿Tiene sentido esto para titulares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'local': 1, 'tcby': 1, 'missed': 1, 'past': 1, '2': 1, 'logo': 1, 'changes': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq = FreqDist(titular_wt_sin_sw)\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Completo\n",
    "\n",
    "Antes de pasar a extraer features de cada instancia del Dataset, podemos hacer un pequeño análisis del dataset en su conjunto. Por ejemplo, una opción es agrupar todos los titulares por tipo y extraer de cada clase las palabras más frecuentes. Para ello:\n",
    "\n",
    "1. Agrupar los titulares por tipo. Crear un dataframe para cada uno. Recuerden usar máscaras.\n",
    "2. Crear una lista vacia y agregar en esa lista todos los titulares (por tipo/dataframe creado) ya tokenizados (usar el `RegexpTokenizer`) y filtrado por `stopwords`.\n",
    "3. Usar el `FreqDist` en esa lista que acaban de llenar. Llevar lo que devuelve `FreqDist` a un Dataframe. Ordenar por frecuencia en que aparece cada palabra.\n",
    "4. Hacer un `barplot` o similar para visualizar.\n",
    "5. ¿Qué palabras filtrarían, aparte de las que aparecen en `stopwords`? Crear una lista vacía y agregarlas a mano. Agregar en el código que realizaron una línea (similar a la que usan con `stopwords`) para que también filtre por esas palabras.\n",
    "6. Volver a visualizar.\n",
    "\n",
    "#### No-Sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_sarcasmo = dataset[COMPLETAR]\n",
    "dataset_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_titulares_no_sarcasmo = COMPLETAR\n",
    "for i in range(dataset_no_sarcasmo.shape[0]):\n",
    "    titular = COMPLETAR #seleccionar el titular\n",
    "    titular = COMPLETAR # Tokenizar con RegexpTokenizer\n",
    "    titular = COMPLETAR # Filtrar por stopwords\n",
    "    todos_titulares_no_sarcasmo.COMPLETAR(COMPLETAR) #agregar el resultado a la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda convierte una lista de listas en una unica lista\n",
    "todos_titulares_no_sarcasmo = list(itertools.chain(*todos_titulares_no_sarcasmo))\n",
    "todos_titulares_no_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FreqDist\n",
    "freq_no_sarcasmo = nltk.COMPLETAR(COMPLETAR)\n",
    "freq_no_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# googlear: how to get pandas dataframe from freqdist\n",
    "df_no_sarcasmo = COMPLETAR\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenar por frecuencia\n",
    "df_no_sarcasmo.COMPLETAR(COMPLETAR)\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_sarcasmo.reset_index(drop = True, inplace=True)\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.barplot(x  = df_no_sarcasmo.iloc[:30].Word, y = df_no_sarcasmo.iloc[:30].Frequency)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sarcasmo = COMPLETAR\n",
    "dataset_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_titulares_sarcasmo = COMPLETAR\n",
    "todos_titulares_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sarcasmo = COMPLETAR\n",
    "freq_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarcasmo = COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarcasmo.COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sarcasmo.COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtrar = []\n",
    "if False:\n",
    "    filtrar.append(\"u\")\n",
    "    filtrar.append(\"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming\n",
    "Por razones gramaticales muchas palabras pueden escribirse de manera distinta (debido a conjugación, género, número) pero tener el mismo significado para el texto. Por ejemplo si decimos \"jugar\", \"jugando\" o \"juega\", debido a la como estan conjugadas, la computadora las tratará como palabras distintas, pero en términos de significado, todas estan relacionadas al verbo Jugar. Muchas veces nos va a convenir unir todas estos términos en uno solo.\n",
    "\n",
    "Una de las manera de hacer esto es por \"STEMMING\". El Stemming es un proceso eurístico que recorta la terminación de las palabras, agrupándolas por su raiz. Reduzcamos la cantidad de palabras diferentes en nuestro dataset utilizando este proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Importar nuevamente el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = COMPLETAR\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b, Tomar del `dataset` solo las columnas de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = COMPLETAR\n",
    "dataset.dropna(axis=0,inplace=True)  # Si hay alguna nan, tiramos esa instancia\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Antes de realizar el proceso de Stemming, vamos a normalizar el texto con lo que ya estuvimos viendo. Le agregamos en este caso el uso de la libreria `re`, que nos permite sacar del texto todos los caracteres que no sean palabras. Notemos que hay veces que no conviene quitar estos caracteres ya que, por ejemplo, no podremos distiguir preguntas (?) o exclamaciones (!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "titular_list=[]\n",
    "for titular in dataset.headline:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.COMPLETAR\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [COMPLETAR for COMPLETAR in COMPLETAR if not COMPLETAR in COMPLETAR]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular = [stemmer.stem(COMPLETAR) for COMPLETAR in COMPLETAR]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular = \" \".join(titular)\n",
    "    \n",
    "    # Vamos armando una lista con todos los titulares\n",
    "    titular_list.append(COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Agregamos al dataset una columna llamado `titular_stem` que contenga los titulares stemmizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"titular_stem\"] = COMPLETAR\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos un nuevo dataset llamado `dataset_stem` que contenga solo las columnas `titular_stem` e `is_sarcastic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_stem = COMPLETAR\n",
    "dataset_stem.dropna(axis=0,inplace=True)  # Por si quedaron titulares vacios\n",
    "dataset_stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization\n",
    "\n",
    "Otra manera de llevar distintas palabras a un raíz común en la que comparten un significado es mediante el procezo de 'Lemmatizar' el texto. Es similar al 'Stemming' pero un poco más educado, ya que intenta realizar el proceso teniendo en cuenta cuál es el rol que la palabra cumple en el texto. Esto quiere decir que su accionar será distinto si la palabra a lemmantizar está actuando como verbo, sustantivo, etc. \n",
    "\n",
    "Para usar las funciones que ofrece `nltk` para lemmantizar, tendremos primero que descargar la libreria `Wordnet` que se encuentra en la solapa 'corpora' y las librerias 'maxent_treebank_pos_' y 'averaged_perceptron_tagger' que se encuentra en la solapa 'Models'. Para eso ejecute la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo actúa el lemmatizer sobre una frase de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracion que usaremos como ejemplo\n",
    "frase = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Hay que regularizar el texto. Dejar solo letra, pasar a minúsculas y tokenizar:\n",
    "\n",
    "# Sacamos todo lo que no sean letras\n",
    "frase = COMPLETAR\n",
    "# Pasamos a minúsculas\n",
    "frase = COMPLETAR\n",
    "# Tokenizamos\n",
    "frase_tokens = COMPLETAR\n",
    "\n",
    "# Veamos como cambians las palabras al lemmatizar\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for palabra in frase_tokens:\n",
    "    print (\"{0:20}{1:20}\".format(palabra,wordnet_lemmatizer.lemmatize(palabra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Les sorprende lo que paso? No cambiaron casi nada (solo se fueron las \"s\" del final). El problema es que precisamos darle información extra al Lemmatizer, decirle qué rol está cumpliendo la palabra en la oración. Si se fijan en la documentación, esto se hace pasandole un argumento extra a la función llamado POS (Part Of Speech).\n",
    "\n",
    "Hay distintos metodos que intentan averiguar el rol que cumple una palabra en una oración. Nosotros vamos a utilizar uno que viene incorporado en NLTK llamado pos_tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.pos_tag(frase_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas refieren al tipo de palabra. Vamos a definir una función para traducir estas etiquetas a los valores de POS que entiende 'wordnet_lemmatizer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos finalmente como funciona en nuestro ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(frase)]\n",
    "tipo_palabra = [get_wordnet_pos(w) for w in nltk.word_tokenize(frase)]\n",
    "\n",
    "# Veamos como cambiaron las palabras\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Palabra:\",\"Tipo:\",\"Lemma:\"))\n",
    "for i in range(len(frase_tokens)):\n",
    "    print (\"{0:20}{1:20}{2:20}\".format(frase_tokens[i],tipo_palabra[i],frase_lemma[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Ahora les toca aplicar todo esto a nuestro dataset. Vamos a volver a importarlo y hacer un procedimiento análogo al que hicimos para la parte de Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "dataset = COMPLETAR\n",
    "dataset = COMPLETAR\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titular_list=[]\n",
    "for titular in dataset.headline:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular = COMPLETAR\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular = COMPLETAR\n",
    "    # Tokenizamos para separar las palabras\n",
    "    titular = COMPLETAR\n",
    "    \n",
    "    # Aplicamos el Lemmatizer (Esto puede tardar un ratito)\n",
    "    frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in titular]\n",
    "    \n",
    "    \n",
    "    # Eliminamos las palabras d emenos de 3 letras\n",
    "    titular = COMPLETAR\n",
    "    # Sacamos las Stopwords\n",
    "    titular = COMPLETAR\n",
    "    \n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    #dataset[\"titular_normalizado\"] = titular_list\n",
    "    titular_list.append(titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"titular_lemm\"] = COMPLETAR\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Por último nos armamos un nuevo datasate llamado `dataset_lem` que tenga solo las columnas `titular_lemm` y `is_sarcastic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemm = COMPLETAR\n",
    "dataset_lemm.dropna(axis=0,inplace=True)  # Por si quedaron titulares vacios\n",
    "dataset_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vectorizar\n",
    "Tenemos un grupo de palabras por cada titular (bag of words). La idea ahora es representar esta lista de palabras como un vector. Para esto vamos a utilizar la función CountVectorizer de sklearn. Esta función nos permite representar cada título por un vector con un `1` en las palabras que contiene y un `0` en las que no. Además, vamos a trabajar únicamente con las palabras que aparecen más veces en el texto, ya que las que aparecen una única vez o pocas veces no nos van a brindar información que se pueda generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Tomamos la lista de palabras y el vector que nos dice si es o no sarcástico el título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamso la lista de palabras y el vector que nos dice si es o no sarcastico el titulo\n",
    "list_titulos = list(COMPLETAR)\n",
    "is_sarc = COMPLETAR\n",
    "\n",
    "## Para probar con Stemmizer:\n",
    "#list_titulos = list(dataset_stem'titular_stem'].values)\n",
    "#is_sarc = dataset_stem['is_sarcastic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Preparamos el conversor de bag of words a vectores que traemos de sklearn. `CountVectorizer` posee varias funcionalidades que pueden determinarse a partir de parámetros. Les recomendamos fuertemente leer su documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Usaremos solo las 1000 palabras con mas frecuencia en todo el corpus para generar los vectores\n",
    "max_features=1000\n",
    "\n",
    "# Es decir que cada instancia tendrá 1000 features\n",
    "cou_vec=CountVectorizer(max_features=max_features) # stop_words=\"english\" , ngram_range=(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que desde `CountVectorizer` se pueden quitar las stopwords (algo que ya hicimos con `nltk`) e incluir los n_gramas automáticamente.\n",
    "\n",
    "c. Ahora sí, vamos generarnos los vectores para cada título a partir del corpus total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_titulos = cou_vec.fit_transform(list_titulos)\n",
    "\n",
    "# Tomamos las palabras\n",
    "all_words = cou_vec.get_feature_names()\n",
    "\n",
    "# Vizualizamos las 50 palabras mas usadas\n",
    "print(\"50 palabras mas usadas: \",all_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modelar\n",
    "\n",
    "Ahroa sí estamos listos para usar todo nuestro conocimiento de modelos en este set de datos. Tengamos en cuenta que, dependiendo el número de palabras (features) que hayamos elegido, los modelos pueden tardar un rato en entrenarse.\n",
    "\n",
    "a. Primero, como siempre, separamos en test y train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = matriz_titulos.toarray()\n",
    "y = is_sarc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien definimos una función que nos permite plotear los resultados en una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def confusion(ytest,y_pred):\n",
    "    names=[\"No Sarcastico\",\"Sarcastico\"]\n",
    "    cm=confusion_matrix(ytest,y_pred)\n",
    "    f,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n",
    "    plt.xlabel(\"y_pred\")\n",
    "    plt.ylabel(\"y_true\")\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "a. Empecemos por un simple Naive Bayes para tener un benchmark de referencia para el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(xtrain,ytrain)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Veamos cómo queda graficada la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "a. Veamos cómo funciona un random forest para predecir el sarcasmo de una nota en base a su titular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "rf = COMPLETAR\n",
    "rf.fit(COMPLETAR)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Grafiquen su matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Habíamos dicho que algo muy bueno de Random Forest era poder preguntarle por la importancia de los features que uso para clasificar. Veamos en este caso cuales son las palabras que mayormente determinan el sarcasmo de una nota para este clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le preguntamos la importancia de cada feature (cada palabra)\n",
    "importances = rf.feature_importances_\n",
    "# Tomamos la lista de palabras\n",
    "all_words = cou_vec.get_feature_names()\n",
    "columns = all_words\n",
    "\n",
    "# Ordenamos por importnacia y tomamos las 20 primeras\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices = indices[:20]\n",
    "selected_columns = [columns[i] for i in indices]\n",
    "selected_importances = importances[indices]\n",
    "\n",
    "# Por ultimo graficamos\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.barplot(selected_columns, selected_importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Por último vamos a utilizar uno de los modelos mas prometedores para este tipo de datos donde el numero de features es comparable al número de instancias: SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Entrene un modelo de SVM Lineal y calcule su accuracy para C = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notar que en vez de utilizar SVC, vamos a usar LinearSVC. \n",
    "# Para el Kernel Lineal, esta función es MUCHO mas rapida que la tradicional SVC.\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C = 1)\n",
    "svc.fit(COMPLETAR)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Grafiquen su matrz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore las posibilidades\n",
    "\n",
    "Si llegaron hasta acá, ya cuentan con todas las herramientas para poder explorar que sucede con el poder predictivo cuando van cambiando la manera en que procesan y vectorizan el texto. Algunas sugerencias para explorar son las siguientes:\n",
    "\n",
    "a) Pruebe con Stemmizar en vez de lemmantizar\n",
    "\n",
    "b) Cambie el numero de features que esta tomando.\n",
    "\n",
    "c) Incluya los 2-gramas.\n",
    "\n",
    "d) Conserve los signos de exclamación y pregunta del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio Extra\n",
    "\n",
    "\"Scrapear\" una nota de algún medio digital y procesarla utilizando las herramientas que vimos. Para ver como importar el texto de la pagina de la noticia, ver video 1.6 'Frecuencia de palabras' de Procesamiento de Lenguaje Natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias que van a precisar para esto\n",
    "from bs4 import BeutifulSoup\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
